{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372ae0bb-a2f7-4d71-9cf5-8f9b551cc8e6",
   "metadata": {},
   "source": [
    "# 1. Setup and Configuration\n",
    "- Loads necessary libraries\n",
    "- Defines some model hyperparameters and training configurations\n",
    "- Specifies the path to the processed data file from the preprocessing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7412543-d1af-4e94-9d50-7320bfd54b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter, deque\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d97399db-e0ad-459b-acb7-9c32d387aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_PICKLE_PATH = 'processed_cache_data.pkl'\n",
    "SEQ_LENGTH = 20\n",
    "MODEL_MAX_SEQ_LENGTH = 50\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 5\n",
    "K_PREFETCH_MODEL = 1\n",
    "LRU_CACHE_SIZE_PERCENTAGE = 0.001\n",
    "GRAD_CLIP = 1.0\n",
    "TRAIN_SPLIT_RATIO = 0.8\n",
    "NUM_WORKERS_DATALOADER = 8\n",
    "NUM_INIT_WORKERS_DATASET = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5927a9f0-f38b-41d3-b43e-bf1a08b2142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276a5ea3-67b1-496f-86ce-76423806a4d3",
   "metadata": {},
   "source": [
    "# 2. Define Transformer Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fe8a7c-26b4-4e9a-ad74-89b337c879af",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Kelas ini mengimplementasikan mekanisme positional encoding dari transformer\n",
    "Mekanisme ini akan menambahkan informasi mengenai posisi dari token pada sequence masukan\n",
    "Mekanisme ini penting karena transformer memroses token secara parallel sehingga perlu diketahui konteks posisi dari token\n",
    "'''\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Ditambahkan dropout untuk mencegah overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Tensor ini akan merepresentasikan posisi token\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "\n",
    "        # Menghitung pembagi dari fungsi sinus dan cosinus yang akan digunakan pada positional encoding\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Inisialisasi tensor positional encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Nilai dari positional encoding untuk elemen berindeks ganjil adalah cosinus dan genap adalah sinus\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Memasukkan positional encoding sebagai buffer --> buffer adalah state dari model yang tidak dilatih\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Menghitung nilai positional encoding dengan menambahkan nilai positional encoding ke tensor masukan\n",
    "        # Setelah itu aplikasikan dropout\n",
    "        x = x + self.pe[:x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ebaaef-1402-4f75-9a7e-3ce55d2ae8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Kelas ini mengimplementasikan multi-head self-attention (MHSA)\n",
    "Mekanisme ini yang memungkinkan transformer untuk mengetahui seberapa penting token lain saat memroses sebuah token\n",
    "'''\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Inisialisasi jumlah attention head dan dimensinya\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Inisialisasi linear layer yang akan memproyeksikan input menjadi Query, Key, dan Value\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Inisialiasi linear layer yang akan menjadi output dari Q, K, dan V\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Digunakan juga dropout untuk mencegah overfitting\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attention_mask: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = query.size(0)\n",
    "        seq_len_q = query.size(1)\n",
    "        seq_len_k = key.size(1)\n",
    "\n",
    "        # Pertama-tama input diproyeksikan menjadi Query, Key, dan Value\n",
    "        # Digunakan view().transpose() untuk mendukung pemrosesan multi-head\n",
    "        Q = self.W_q(query).view(batch_size, seq_len_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, seq_len_k, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_len_k, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Dihitung nilai atensi (kemiripan query ke key) menggunakan scaled dot-product attention\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Apabila ada masking, maka diterapkan\n",
    "        # Masking ini digunakan untuk mencegah decoder dari melihat token di depan (future token)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(attention_mask == True, float('-inf'))\n",
    "\n",
    "        # Terapkan softmax untuk mendapatkan probabilitas / seberapa penting token lain dalam konteks token yang sedang diproses saat ini\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout_attn(attention_weights)\n",
    "\n",
    "        # Hitung context vector\n",
    "        # Hasil vector ini adalah weighted average dari Values\n",
    "        context_vector = torch.matmul(attention_weights, V).transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
    "        output = self.W_o(context_vector)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c9ecf5-fc87-4c2b-bbe7-91966efa8a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implementasi dari mekanisme position wise feed forward\n",
    "Mekanisme / lapisan ini berada tepat setelah MHSA\n",
    "Lapisan ini digunakan untuk menambahkan atau membuat fitur menjadi non-linear\n",
    "'''\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(self.dropout(self.activation(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11925f72-919b-409b-907f-dacbcfd952ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Kelas yang merepresentasikan satu blok decoder pada transformer\n",
    "Kelas ini menggabungkan MHSA dan Position-Wise Feed-Forward\n",
    "'''\n",
    "class DecoderBlockScratch(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        norm_x = self.norm1(x)\n",
    "        attn_output, _ = self.self_attention(norm_x, norm_x, norm_x, attention_mask)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        norm_x = self.norm2(x)\n",
    "        ff_output = self.feed_forward(norm_x)\n",
    "        x = x + self.dropout2(ff_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3976fce8-e238-495a-9dfa-663f1b363bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Kelas yang merepresentasikan decoder yang telah di-assembly menjadi transformer\n",
    "Transformer ini melakukan embedding dengan mengubah token menjadi dense vector\n",
    "'''\n",
    "class DecoderOnlyTransformerScratch(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_heads: int, num_layers: int, d_ff: int, max_seq_length: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlockScratch(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout_emb = nn.Dropout(dropout)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc_out.bias.data.zero_()\n",
    "        self.fc_out.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def _generate_causal_mask(self, size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.triu(torch.ones(size, size, device=device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        batch_size, seq_len = src.shape\n",
    "        device = src.device\n",
    "        emb_out = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.dropout_emb(self.pos_encoder(emb_out))\n",
    "        causal_mask = self._generate_causal_mask(seq_len, device).unsqueeze(0).unsqueeze(0)\n",
    "        if src_key_padding_mask is not None:\n",
    "            expanded_padding_mask = src_key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            causal_mask = (causal_mask | expanded_padding_mask).bool()\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, causal_mask)\n",
    "        return self.fc_out(self.final_norm(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2084ce5f-d286-4bd5-be05-c62f77ecd1e4",
   "metadata": {},
   "source": [
    "# 3. Define `CacheDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efaba17b-8e82-4b79-a066-7498ff0b4c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fungsi ini digunakan untuk menciptakan slices dari sequence\n",
    "Slices yang dihasilkan menggunakan prinsip sliding window\n",
    "'''\n",
    "def _create_single_sequence_pair_for_mp(args_tuple):\n",
    "    indexed_obj_ids_ref, i, sequence_length_val = args_tuple\n",
    "    \n",
    "    input_seq_list = indexed_obj_ids_ref[i : i + sequence_length_val]\n",
    "    target_seq_list = indexed_obj_ids_ref[i + 1 : i + sequence_length_val + 1]\n",
    "    \n",
    "    return torch.tensor(input_seq_list, dtype=torch.long), \\\n",
    "           torch.tensor(target_seq_list, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b042aa-3dd3-4f9d-8c13-a66517760089",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Kelas dataset dari sekuens akses objek\n",
    "Kelas ini akan merubah sekuens masukan menjadi potongan sekuens berbasis sliding window\n",
    "'''\n",
    "class CacheDataset(Dataset):\n",
    "    def __init__(self, filtered_obj_id_sequence: list, list_of_popular_objects: list, sequence_length: int, num_init_workers: int = 0):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.popular_objects_vocab = sorted(list(set(list_of_popular_objects)))\n",
    "        self.obj_to_idx = {obj: i for i, obj in enumerate(self.popular_objects_vocab)}\n",
    "        self.idx_to_obj = {i: obj for obj, i in self.obj_to_idx.items()}\n",
    "        self.vocab_size = len(self.popular_objects_vocab)\n",
    "        \n",
    "        self.indexed_obj_ids = [self.obj_to_idx[obj] for obj in filtered_obj_id_sequence if obj in self.obj_to_idx]\n",
    "        \n",
    "        self.input_sequences = []\n",
    "        self.target_sequences = []\n",
    "        \n",
    "        if len(self.indexed_obj_ids) >= self.sequence_length + 1:\n",
    "            num_total_sequences = len(self.indexed_obj_ids) - self.sequence_length\n",
    "\n",
    "            actual_init_workers = 0\n",
    "            if num_init_workers > 0:\n",
    "                 actual_init_workers = min(num_init_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            \n",
    "            min_sequences_for_parallel = 1000 \n",
    "            min_sequences_per_worker = 50\n",
    "\n",
    "            if actual_init_workers > 0 and \\\n",
    "               num_total_sequences >= min_sequences_for_parallel and \\\n",
    "               (num_total_sequences / actual_init_workers) >= min_sequences_per_worker:\n",
    "                \n",
    "                print(f\"Using {actual_init_workers} workers for CacheDataset sequence creation ({num_total_sequences} sequences).\")\n",
    "                tasks_args = [(self.indexed_obj_ids, i, self.sequence_length) for i in range(num_total_sequences)]\n",
    "\n",
    "                # Bagian ini awalnya untuk memproses dataset secara parallel tetapi masih terdapat bug\n",
    "                # Untuk saat ini gunakan num workers 0 untuk membuat pemrosesan menjadi sekuensial sehingga tidak menghadapi bug\n",
    "                with multiprocessing.Pool(processes=actual_init_workers) as pool:\n",
    "                    results = []\n",
    "                    for pair in tqdm(pool.imap_unordered(_create_single_sequence_pair_for_mp, tasks_args), \n",
    "                                     total=num_total_sequences, \n",
    "                                     desc=\"Creating Dataset Sequences (Parallel)\", \n",
    "                                     unit=\"sequence\", \n",
    "                                     leave=False):\n",
    "                        results.append(pair)\n",
    "                \n",
    "                if results:\n",
    "                    self.input_sequences, self.target_sequences = zip(*results)\n",
    "                    self.input_sequences = list(self.input_sequences)\n",
    "                    self.target_sequences = list(self.target_sequences)\n",
    "            else:\n",
    "                if actual_init_workers > 0:\n",
    "                    print(f\"Dataset size ({num_total_sequences} sequences) or worker load too small for parallel init with {actual_init_workers} workers. Using sequential.\")\n",
    "                for i in tqdm(range(num_total_sequences), desc=\"Creating Dataset Sequences (Sequential)\", unit=\"sequence\", leave=False):\n",
    "                    self.input_sequences.append(torch.tensor(self.indexed_obj_ids[i : i + self.sequence_length], dtype=torch.long))\n",
    "                    self.target_sequences.append(torch.tensor(self.indexed_obj_ids[i + 1 : i + self.sequence_length + 1], dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_sequences[idx], self.target_sequences[idx]\n",
    "    \n",
    "    def get_vocab_info(self):\n",
    "        return {'obj_to_idx': self.obj_to_idx, 'idx_to_obj': self.idx_to_obj, 'vocab_size': self.vocab_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d4fc02-fb50-439e-9bdf-a5ee03e6a9b3",
   "metadata": {},
   "source": [
    "# 4. Define Training and Evaluation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d25ff348-9c8f-4a3f-97b7-7a7939719a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, \n",
    "                optimizer: optim.Optimizer, device: torch.device, grad_clip_value: float = None, epoch_num: int = 0, config_name: str = \"\"):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    # Wrap dataloader with tqdm for batch progress\n",
    "    batch_iterator = tqdm(dataloader, desc=f\"Epoch {epoch_num} Training\", leave=False, unit=\"batch\")\n",
    "    for batch_idx, (input_seqs, target_seqs) in enumerate(batch_iterator):\n",
    "        input_seqs, target_seqs = input_seqs.to(device), target_seqs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_seqs, src_key_padding_mask=None)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), target_seqs.view(-1))\n",
    "        loss.backward()\n",
    "        if grad_clip_value:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_value)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # Update tqdm description with current loss\n",
    "        batch_iterator.set_postfix_str(f\"Loss: {loss.item():.4f}\")\n",
    "    return total_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
    "\n",
    "'''\n",
    "Evaluasi digunakan mensimulasikan performa LRU cache dengan prefetcher\n",
    "Prefetcher yang digunakan hanya melakukan prefetching terhadap satu objek saja\n",
    "'''\n",
    "def evaluate_model_lru_with_prefetcher(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, \n",
    "                                       device: torch.device, model_vocab_size: int, \n",
    "                                       k_items_to_prefetch: int = 1, \n",
    "                                       cache_size_percentage: float = 0.1):\n",
    "    model.eval()\n",
    "    total_model_loss = 0.0 # Loss of the transformer model itself\n",
    "    total_lru_misses = 0\n",
    "    total_lru_accesses = 0\n",
    "    \n",
    "    if model_vocab_size == 0:\n",
    "        print(\"Warning: model_vocab_size is 0. LRU simulation might not be meaningful.\")\n",
    "        cache_capacity = k_items_to_prefetch \n",
    "    else:\n",
    "        cache_capacity = max(1, int(model_vocab_size * cache_size_percentage))\n",
    "    \n",
    "    lru_cache = deque(maxlen=cache_capacity) # Cache persists across batches for a coherent trace\n",
    "\n",
    "    batch_iterator = tqdm(dataloader, desc=\"Evaluating with Prefetcher+LRU\", leave=False, unit=\"batch\")\n",
    "    with torch.no_grad():\n",
    "        for input_seqs, target_seqs in batch_iterator: \n",
    "            input_seqs, target_seqs = input_seqs.to(device), target_seqs.to(device)\n",
    "            outputs = model(input_seqs, src_key_padding_mask=None) \n",
    "            \n",
    "            # Calculate model's prediction loss\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), target_seqs.view(-1))\n",
    "            total_model_loss += loss.item() \n",
    "\n",
    "            # LRU Simulation with Prefetcher\n",
    "            for b in range(input_seqs.size(0)): \n",
    "                for s in range(target_seqs.size(1)): \n",
    "                    current_logits_for_prefetch = outputs[b, s, :]\n",
    "                    actual_demanded_item_idx = target_seqs[b, s].item()\n",
    "\n",
    "                    # Prefetch action\n",
    "                    if k_items_to_prefetch > 0:\n",
    "                        _, predicted_indices_to_prefetch = torch.topk(\n",
    "                            current_logits_for_prefetch, \n",
    "                            k=min(k_items_to_prefetch, model_vocab_size if model_vocab_size > 0 else k_items_to_prefetch), \n",
    "                            dim=-1\n",
    "                        )\n",
    "                        for pred_idx_tensor in predicted_indices_to_prefetch:\n",
    "                            pred_idx = pred_idx_tensor.item()\n",
    "                            if pred_idx in lru_cache:\n",
    "                                lru_cache.remove(pred_idx) \n",
    "                            lru_cache.append(pred_idx) \n",
    "                    \n",
    "                    # Demand action\n",
    "                    total_lru_accesses += 1\n",
    "                    if actual_demanded_item_idx in lru_cache: # Hit\n",
    "                        lru_cache.remove(actual_demanded_item_idx) \n",
    "                        lru_cache.append(actual_demanded_item_idx)\n",
    "                    else: # Miss\n",
    "                        total_lru_misses += 1\n",
    "                        if actual_demanded_item_idx in lru_cache: \n",
    "                            lru_cache.remove(actual_demanded_item_idx)\n",
    "                        lru_cache.append(actual_demanded_item_idx)\n",
    "            \n",
    "    avg_model_loss_per_batch = total_model_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
    "    lru_miss_ratio = total_lru_misses / total_lru_accesses if total_lru_accesses > 0 else 0.0\n",
    "    return avg_model_loss_per_batch, lru_miss_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8800ab8-f494-44bd-b8a3-f3e3ce69bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Bagian ini akan mengevaluasi performa cache LRU tanpa prefetcher sebagai baseline\n",
    "'''\n",
    "def evaluate_lru_only_cache(dataloader: DataLoader, device: torch.device, \n",
    "                            model_vocab_size: int, cache_size_percentage: float = 0.1):\n",
    "    total_lru_misses = 0\n",
    "    total_lru_accesses = 0\n",
    "\n",
    "    if model_vocab_size == 0:\n",
    "        print(\"Warning: model_vocab_size is 0 for LRU-only. Cache capacity might be 0.\")\n",
    "        cache_capacity = 1\n",
    "    else:\n",
    "        cache_capacity = max(1, int(model_vocab_size * cache_size_percentage))\n",
    "    \n",
    "    print(f\"Simulating Baseline LRU cache with capacity: {cache_capacity} ({cache_size_percentage*100:.1f}% of vocab {model_vocab_size}).\")\n",
    "    \n",
    "    lru_cache = deque(maxlen=cache_capacity)\n",
    "\n",
    "    batch_iterator = tqdm(dataloader, desc=\"Evaluating Baseline LRU\", leave=False, unit=\"batch\")\n",
    "    with torch.no_grad(): # Untuk mencegah terjadi training saja, sebenarnya tidak terlalu diperlukan\n",
    "        for _, target_seqs in batch_iterator:\n",
    "            target_seqs = target_seqs.to(device)\n",
    "\n",
    "            for b in range(target_seqs.size(0)):\n",
    "                for s in range(target_seqs.size(1)):\n",
    "                    actual_demanded_item_idx = target_seqs[b, s].item()\n",
    "                    \n",
    "                    total_lru_accesses += 1\n",
    "\n",
    "                    if actual_demanded_item_idx in lru_cache:\n",
    "                        # Cache Hit\n",
    "                        lru_cache.remove(actual_demanded_item_idx) # Move to MRU\n",
    "                        lru_cache.append(actual_demanded_item_idx)\n",
    "                    else:\n",
    "                        # Cache Miss\n",
    "                        total_lru_misses += 1\n",
    "                        if actual_demanded_item_idx in lru_cache:\n",
    "                            lru_cache.remove(actual_demanded_item_idx)\n",
    "                        lru_cache.append(actual_demanded_item_idx)\n",
    "            \n",
    "    lru_miss_ratio = total_lru_misses / total_lru_accesses if total_lru_accesses > 0 else 0.0\n",
    "    return lru_miss_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7366803d-31cd-4fc4-bd1c-c201844d2e68",
   "metadata": {},
   "source": [
    "# 5. Load Processed Data and Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b531771-8b83-40d4-a856-86cb73937c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from processed_cache_data.pkl...\n",
      "Loaded filtered sequence of length: 15000\n",
      "Number of unique popular objects for vocabulary: 7588\n",
      "Training sequence length: 12000\n",
      "Validation sequence length: 3000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading processed data from {PROCESSED_DATA_PICKLE_PATH}...\")\n",
    "if not os.path.exists(PROCESSED_DATA_PICKLE_PATH):\n",
    "    print(f\"Error: Processed data file not found at {PROCESSED_DATA_PICKLE_PATH}.\")\n",
    "    print(\"Please run the preprocessing notebook first to generate this file.\")\n",
    "    raise FileNotFoundError(f\"Missing {PROCESSED_DATA_PICKLE_PATH}\")\n",
    "\n",
    "with open(PROCESSED_DATA_PICKLE_PATH, 'rb') as f:\n",
    "    processed_data = pickle.load(f)\n",
    "\n",
    "filtered_sequence = processed_data['filtered_sequence_popular_obj_ids']\n",
    "list_of_popular_objects_for_vocab = processed_data['list_of_popular_obj_ids']\n",
    "\n",
    "print(f\"Loaded filtered sequence of length: {len(filtered_sequence)}\")\n",
    "print(f\"Number of unique popular objects for vocabulary: {len(list_of_popular_objects_for_vocab)}\")\n",
    "\n",
    "if not filtered_sequence or not list_of_popular_objects_for_vocab:\n",
    "    print(\"Error: Loaded data is empty. Cannot proceed with training.\")\n",
    "    raise ValueError(\"Empty data loaded from pickle file.\")\n",
    "\n",
    "# Split the filtered sequence for training and validation\n",
    "split_idx = int(TRAIN_SPLIT_RATIO * len(filtered_sequence))\n",
    "train_filtered_ids = filtered_sequence[:split_idx]\n",
    "val_filtered_ids = filtered_sequence[split_idx:]\n",
    "\n",
    "print(f\"Training sequence length: {len(train_filtered_ids)}\")\n",
    "print(f\"Validation sequence length: {len(val_filtered_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98d273d8-a8aa-400c-8c01-0a4477128003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Training Dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5909ae6fbddb47e68869a142236a61d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Dataset Sequences (Sequential):   0%|          | 0/11980 [00:00<?, ?sequence/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Validation Dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af86ccd9b7d46d098f54bc5780629b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Dataset Sequences (Sequential):   0%|          | 0/2980 [00:00<?, ?sequence/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Creating Training Dataset...\")\n",
    "train_dataset = CacheDataset(train_filtered_ids, list_of_popular_objects_for_vocab, SEQ_LENGTH, num_init_workers=NUM_INIT_WORKERS_DATASET)\n",
    "print(\"Creating Validation Dataset...\")\n",
    "val_dataset = CacheDataset(val_filtered_ids, list_of_popular_objects_for_vocab, SEQ_LENGTH, num_init_workers=NUM_INIT_WORKERS_DATASET)\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    raise ValueError(\"Training dataset is empty after processing. Insufficient data for SEQ_LENGTH.\")\n",
    "if len(val_dataset) == 0:\n",
    "    print(\"Warning: Validation dataset is empty. Evaluation will be skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59ee97cd-8f69-40a7-b336-a9ae0625057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pin_memory_flag = True if device.type == 'cuda' else False\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=NUM_WORKERS_DATALOADER, pin_memory=pin_memory_flag)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS_DATALOADER, pin_memory=pin_memory_flag) if len(val_dataset) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4827c86-971c-4926-8181-8dc03b4bb104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective Vocabulary size for all models: 7588\n",
      "Number of training sequences: 11980\n",
      "Number of validation sequences: 2980\n"
     ]
    }
   ],
   "source": [
    "MODEL_VOCAB_SIZE = train_dataset.vocab_size\n",
    "print(f\"Effective Vocabulary size for all models: {MODEL_VOCAB_SIZE}\")\n",
    "print(f\"Number of training sequences: {len(train_dataset)}\")\n",
    "print(f\"Number of validation sequences: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f449728-86e1-457d-8456-67626cc914e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating Baseline LRU cache with capacity: 7 (0.1% of vocab 7588).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e411dcbbeb9947f3950a4c58e063e76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Baseline LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline LRU-Only Cache Miss Ratio (on validation set): 0.7043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_lru_miss_ratio = float('nan')\n",
    "if val_dataloader and len(val_dataset) > 0:\n",
    "    if MODEL_VOCAB_SIZE > 0:\n",
    "        baseline_lru_miss_ratio = evaluate_lru_only_cache(\n",
    "            val_dataloader, device, \n",
    "            model_vocab_size=MODEL_VOCAB_SIZE, \n",
    "            cache_size_percentage=LRU_CACHE_SIZE_PERCENTAGE\n",
    "        )\n",
    "        print(f\"\\nBaseline LRU-Only Cache Miss Ratio (on validation set): {baseline_lru_miss_ratio:.4f}\\n\")\n",
    "    else:\n",
    "        print(\"\\nMODEL_VOCAB_SIZE is 0, cannot run baseline LRU-only cache evaluation meaningfully.\")\n",
    "else:\n",
    "    print(\"\\nValidation dataloader not available, skipping baseline LRU-only cache evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca48cb-8824-4cad-85ac-c9d51f77d1de",
   "metadata": {},
   "source": [
    "# 6. Define Hyperparameter Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4623ebac-ca76-4838-85e0-584b0e4889c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_configs = [\n",
    "    {\n",
    "        \"name\": \"Config1_Baseline\",\n",
    "        \"D_MODEL\": 128,\n",
    "        \"NUM_HEADS\": 4,\n",
    "        \"NUM_LAYERS\": 3,\n",
    "        \"D_FF\": 256,\n",
    "        \"DROPOUT\": 0.1,\n",
    "        \"LEARNING_RATE\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Config2_LargerModel_LowerLR\",\n",
    "        \"D_MODEL\": 256, # Larger model\n",
    "        \"NUM_HEADS\": 8,  # Must be divisor of D_MODEL\n",
    "        \"NUM_LAYERS\": 4, # Deeper model\n",
    "        \"D_FF\": 512,   # Larger FFN\n",
    "        \"DROPOUT\": 0.15, # Slightly more dropout for larger model\n",
    "        \"LEARNING_RATE\": 0.0005, # Lower LR for potentially more stable training\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Config3_SmallerModel_HigherLR_MoreDropout\",\n",
    "        \"D_MODEL\": 64,  # Smaller model\n",
    "        \"NUM_HEADS\": 2,  # Must be divisor of D_MODEL\n",
    "        \"NUM_LAYERS\": 2, # Shallower model\n",
    "        \"D_FF\": 128,   # Smaller FFN\n",
    "        \"DROPOUT\": 0.2,  # Higher dropout\n",
    "        \"LEARNING_RATE\": 0.002, # Higher LR\n",
    "    },\n",
    "    # Add a fourth configuration\n",
    "    {\n",
    "        \"name\": \"Config4_MediumModel_VariedHeads\",\n",
    "        \"D_MODEL\": 128,\n",
    "        \"NUM_HEADS\": 8,  # More heads for same D_MODEL\n",
    "        \"NUM_LAYERS\": 3,\n",
    "        \"D_FF\": 256,\n",
    "        \"DROPOUT\": 0.1,\n",
    "        \"LEARNING_RATE\": 0.001,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598b3ad-439a-4dcc-9bd3-be1343661da7",
   "metadata": {},
   "source": [
    "# 7. Training and Evaluation Loop for Each Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e8293db-6339-4732-be67-87dcb8b6f641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40601fd61c784e5eb75bbf9cb1bc40b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configurations:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== Starting Training for: Config1_Baseline ====================\n",
      "Hyperparameters: {'name': 'Config1_Baseline', 'D_MODEL': 128, 'NUM_HEADS': 4, 'NUM_LAYERS': 3, 'D_FF': 256, 'DROPOUT': 0.1, 'LEARNING_RATE': 0.001}\n",
      "Model for Config1_Baseline initialized with 2347812 trainable parameters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3394687a4a264855a3abf0c5569c4ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Config 'Config1_Baseline' Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de83549266f4d6fba4131deb57d5b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config1_Baseline, Epoch 1 Training: Avg Model Loss: 2.8173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f9700c291b416e8e89d03e7bd4fe5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config1_Baseline, Epoch 1 Validation: Avg Model Loss: 12.1582, Prefetcher+LRU Miss Ratio: 0.6895\n",
      "Config: Config1_Baseline, Epoch 1 duration: 78.79 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45791f971e044519a0a10ea77ea86c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config1_Baseline, Epoch 2 Training: Avg Model Loss: 0.4926\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb01a89cacea4522a99f144a8a22c784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config1_Baseline, Epoch 2 Validation: Avg Model Loss: 14.4213, Prefetcher+LRU Miss Ratio: 0.6854\n",
      "Config: Config1_Baseline, Epoch 2 duration: 78.31 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2b911a87ec4a39a2216d73cd77370b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config1_Baseline, Epoch 3 Training: Avg Model Loss: 0.3853\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a43d2f7a324e9f9b858a8d8cbeb406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config1_Baseline, Epoch 3 Validation: Avg Model Loss: 14.4320, Prefetcher+LRU Miss Ratio: 0.6877\n",
      "Config: Config1_Baseline, Epoch 3 duration: 65.96 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71eba6c9137f4f4db222cd28fd31394e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config1_Baseline, Epoch 4 Training: Avg Model Loss: 0.3457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eebe756fa964aeb8cf2d3fc4954cb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config1_Baseline, Epoch 4 Validation: Avg Model Loss: 14.7113, Prefetcher+LRU Miss Ratio: 0.6873\n",
      "Config: Config1_Baseline, Epoch 4 duration: 78.23 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ad068ee89d4737ac5b30515efaa21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config1_Baseline, Epoch 5 Training: Avg Model Loss: 0.3173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c3f9fc976a418bac80c9c63dcc7dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config1_Baseline, Epoch 5 Validation: Avg Model Loss: 15.5823, Prefetcher+LRU Miss Ratio: 0.6835\n",
      "Config: Config1_Baseline, Epoch 5 duration: 69.28 seconds\n",
      "\n",
      "Training for Config1_Baseline completed in 370.71 seconds.\n",
      "Best Validation Prefetcher+LRU Miss Ratio for Config1_Baseline: 0.6835 at Epoch 5\n",
      "\n",
      "\n",
      "==================== Starting Training for: Config2_LargerModel_LowerLR ====================\n",
      "Hyperparameters: {'name': 'Config2_LargerModel_LowerLR', 'D_MODEL': 256, 'NUM_HEADS': 8, 'NUM_LAYERS': 4, 'D_FF': 512, 'DROPOUT': 0.15, 'LEARNING_RATE': 0.0005}\n",
      "Model for Config2_LargerModel_LowerLR initialized with 6001572 trainable parameters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707645aa403e420c973b72031d73cbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Config 'Config2_LargerModel_LowerLR' Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adc8dcfa1844606a6d5fe8845766a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config2_LargerModel_LowerLR, Epoch 1 Training: Avg Model Loss: 2.9584\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48dd4147d2641fa9253598f35b3926e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config2_LargerModel_LowerLR, Epoch 1 Validation: Avg Model Loss: 11.9500, Prefetcher+LRU Miss Ratio: 0.6831\n",
      "Config: Config2_LargerModel_LowerLR, Epoch 1 duration: 88.08 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f82090c6c9496ab33ffd630962e5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config2_LargerModel_LowerLR, Epoch 2 Training: Avg Model Loss: 0.5028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772f95769e004a80b170fda9c386acfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config2_LargerModel_LowerLR, Epoch 2 Validation: Avg Model Loss: 13.3720, Prefetcher+LRU Miss Ratio: 0.6851\n",
      "Config: Config2_LargerModel_LowerLR, Epoch 2 duration: 87.94 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9664990708774c71bcb900cf39cc8e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config2_LargerModel_LowerLR, Epoch 3 Training: Avg Model Loss: 0.3728\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b248954d25dc4b9cb37529be0a1e9337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config2_LargerModel_LowerLR, Epoch 3 Validation: Avg Model Loss: 13.7118, Prefetcher+LRU Miss Ratio: 0.6895\n",
      "Config: Config2_LargerModel_LowerLR, Epoch 3 duration: 87.99 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ecb5b15ce749e0a2ebd008512f3f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config2_LargerModel_LowerLR, Epoch 4 Training: Avg Model Loss: 0.3316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfce39fa40141acb8e49298376b5d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config2_LargerModel_LowerLR, Epoch 4 Validation: Avg Model Loss: 13.6965, Prefetcher+LRU Miss Ratio: 0.6956\n",
      "Config: Config2_LargerModel_LowerLR, Epoch 4 duration: 87.91 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0738ff2312fd43d690b65278d67177a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config2_LargerModel_LowerLR, Epoch 5 Training: Avg Model Loss: 0.3064\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15d75e868774176b3f1b84764303e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config2_LargerModel_LowerLR, Epoch 5 Validation: Avg Model Loss: 14.2879, Prefetcher+LRU Miss Ratio: 0.6876\n",
      "Config: Config2_LargerModel_LowerLR, Epoch 5 duration: 87.76 seconds\n",
      "\n",
      "Training for Config2_LargerModel_LowerLR completed in 439.95 seconds.\n",
      "Best Validation Prefetcher+LRU Miss Ratio for Config2_LargerModel_LowerLR: 0.6831 at Epoch 1\n",
      "\n",
      "\n",
      "==================== Starting Training for: Config3_SmallerModel_HigherLR_MoreDropout ====================\n",
      "Hyperparameters: {'name': 'Config3_SmallerModel_HigherLR_MoreDropout', 'D_MODEL': 64, 'NUM_HEADS': 2, 'NUM_LAYERS': 2, 'D_FF': 128, 'DROPOUT': 0.2, 'LEARNING_RATE': 0.002}\n",
      "Model for Config3_SmallerModel_HigherLR_MoreDropout initialized with 1045924 trainable parameters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a099ff045d045eeb8aa6fd6486fd66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Config 'Config3_SmallerModel_HigherLR_MoreDropout' Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7213070ae4d46419d2a727a8b1c8572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 1 Training: Avg Model Loss: 3.6979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c69b611c93b4b23b7514b2129ca9b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 1 Validation: Avg Model Loss: 15.0378, Prefetcher+LRU Miss Ratio: 0.6788\n",
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 1 duration: 65.63 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0437a637825c4f519ec6c989808952cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 2 Training: Avg Model Loss: 1.1161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43284db5c0844eabaeb15542e125d43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 2 Validation: Avg Model Loss: 16.9838, Prefetcher+LRU Miss Ratio: 0.6847\n",
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 2 duration: 65.86 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f6e54aefce4f20ad8139b4a9977060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 3 Training: Avg Model Loss: 0.7989\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3093f1c71854e2d9efb28f6f5f37bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 3 Validation: Avg Model Loss: 17.6755, Prefetcher+LRU Miss Ratio: 0.6836\n",
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 3 duration: 65.69 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459f941bd89b4c75bef9695f1ca4dfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 4 Training: Avg Model Loss: 0.6975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310c83c3a40c4b559c2af4faaf841d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 4 Validation: Avg Model Loss: 17.9045, Prefetcher+LRU Miss Ratio: 0.6870\n",
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 4 duration: 65.29 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d37c97f3ed47ff8a8949e0828c57f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 5 Training: Avg Model Loss: 0.6417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b47050dfc644769c39135ca9a116d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 5 Validation: Avg Model Loss: 17.9205, Prefetcher+LRU Miss Ratio: 0.6949\n",
      "Config: Config3_SmallerModel_HigherLR_MoreDropout, Epoch 5 duration: 65.31 seconds\n",
      "\n",
      "Training for Config3_SmallerModel_HigherLR_MoreDropout completed in 327.87 seconds.\n",
      "Best Validation Prefetcher+LRU Miss Ratio for Config3_SmallerModel_HigherLR_MoreDropout: 0.6788 at Epoch 1\n",
      "\n",
      "\n",
      "==================== Starting Training for: Config4_MediumModel_VariedHeads ====================\n",
      "Hyperparameters: {'name': 'Config4_MediumModel_VariedHeads', 'D_MODEL': 128, 'NUM_HEADS': 8, 'NUM_LAYERS': 3, 'D_FF': 256, 'DROPOUT': 0.1, 'LEARNING_RATE': 0.001}\n",
      "Model for Config4_MediumModel_VariedHeads initialized with 2347812 trainable parameters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41167f74653449c6a7fbdc1f41c949c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Config 'Config4_MediumModel_VariedHeads' Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15c42fc1f1d4665888e87beac0939ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config4_MediumModel_VariedHeads, Epoch 1 Training: Avg Model Loss: 2.8168\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da03c37be006454e99f92f4e1a0da09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config4_MediumModel_VariedHeads, Epoch 1 Validation: Avg Model Loss: 12.1778, Prefetcher+LRU Miss Ratio: 0.6929\n",
      "Config: Config4_MediumModel_VariedHeads, Epoch 1 duration: 76.60 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3375ca4ad0cd4d569464d3089134bd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config4_MediumModel_VariedHeads, Epoch 2 Training: Avg Model Loss: 0.4917\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e077efbe9d2340f5a3b00796bdb28d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config4_MediumModel_VariedHeads, Epoch 2 Validation: Avg Model Loss: 13.8752, Prefetcher+LRU Miss Ratio: 0.6898\n",
      "Config: Config4_MediumModel_VariedHeads, Epoch 2 duration: 76.74 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059b0b5497b0494fa5609b0a7c9ba0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config4_MediumModel_VariedHeads, Epoch 3 Training: Avg Model Loss: 0.3816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a0cf6e89984019ac14674b38ba32e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config4_MediumModel_VariedHeads, Epoch 3 Validation: Avg Model Loss: 14.4821, Prefetcher+LRU Miss Ratio: 0.6906\n",
      "Config: Config4_MediumModel_VariedHeads, Epoch 3 duration: 78.64 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedab5a15b1643a7b34ca2509de04c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config4_MediumModel_VariedHeads, Epoch 4 Training: Avg Model Loss: 0.3407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75208245ee543dfb7d9fdc9437bdb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config4_MediumModel_VariedHeads, Epoch 4 Validation: Avg Model Loss: 14.9057, Prefetcher+LRU Miss Ratio: 0.6886\n",
      "Config: Config4_MediumModel_VariedHeads, Epoch 4 duration: 78.26 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de13b37c64a42d8b4a7b0c35ad1b889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Training:   0%|          | 0/1497 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config4_MediumModel_VariedHeads, Epoch 5 Training: Avg Model Loss: 0.3104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709fa6ed17b14366aacb6f44524e8e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating with Prefetcher+LRU:   0%|          | 0/373 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config4_MediumModel_VariedHeads, Epoch 5 Validation: Avg Model Loss: 15.0231, Prefetcher+LRU Miss Ratio: 0.6922\n",
      "Config: Config4_MediumModel_VariedHeads, Epoch 5 duration: 66.73 seconds\n",
      "\n",
      "Training for Config4_MediumModel_VariedHeads completed in 377.12 seconds.\n",
      "Best Validation Prefetcher+LRU Miss Ratio for Config4_MediumModel_VariedHeads: 0.6886 at Epoch 4\n",
      "\n",
      "\n",
      "==================== All Configurations Processed ====================\n"
     ]
    }
   ],
   "source": [
    "all_results = [] \n",
    "\n",
    "configurations_pbar = tqdm(hyperparameter_configs, desc=\"Configurations\")\n",
    "for config in configurations_pbar: \n",
    "    print(f\"\\n\\n{'='*20} Starting Training for: {config['name']} {'='*20}\")\n",
    "    print(f\"Hyperparameters: {config}\")\n",
    "    start_time_config = time.time()\n",
    "\n",
    "    current_model = DecoderOnlyTransformerScratch(\n",
    "        vocab_size=MODEL_VOCAB_SIZE,\n",
    "        d_model=config['D_MODEL'],\n",
    "        num_heads=config['NUM_HEADS'],\n",
    "        num_layers=config['NUM_LAYERS'],\n",
    "        d_ff=config['D_FF'],\n",
    "        max_seq_length=MODEL_MAX_SEQ_LENGTH,\n",
    "        dropout=config['DROPOUT']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = optim.Adam(current_model.parameters(), lr=config['LEARNING_RATE'])\n",
    "\n",
    "    num_params = sum(p.numel() for p in current_model.parameters() if p.requires_grad)\n",
    "    print(f\"Model for {config['name']} initialized with {num_params} trainable parameters.\")\n",
    "\n",
    "    config_results = {\n",
    "        \"config_name\": config['name'],\n",
    "        \"hyperparameters\": config,\n",
    "        \"num_parameters\": num_params,\n",
    "        \"train_losses\": [],\n",
    "        \"val_model_losses\": [], \n",
    "        \"val_lru_miss_ratios\": [],\n",
    "        \"best_val_lru_miss_ratio\": float('inf'), \n",
    "        \"best_epoch\": -1\n",
    "    }\n",
    "\n",
    "    if len(train_dataloader) == 0:\n",
    "        print(f\"Training dataloader is empty for {config['name']}. Skipping training for this config.\")\n",
    "        all_results.append(config_results) \n",
    "        continue\n",
    "\n",
    "    for epoch in tqdm(range(1, NUM_EPOCHS + 1), desc=f\"Config '{config['name']}' Epochs\", leave=False):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        avg_train_loss = train_epoch(current_model, train_dataloader, criterion, optimizer, device, GRAD_CLIP, epoch_num=epoch, config_name=config['name'])\n",
    "        config_results[\"train_losses\"].append(avg_train_loss)\n",
    "        print(f\"Config: {config['name']}, Epoch {epoch} Training: Avg Model Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        current_val_model_loss = float('nan')\n",
    "        current_val_lru_miss_ratio = float('nan')\n",
    "\n",
    "        if val_dataloader and len(val_dataset) > 0:\n",
    "            avg_val_model_loss, val_lru_miss_ratio_wp = evaluate_model_lru_with_prefetcher(\n",
    "                current_model, val_dataloader, criterion, device, \n",
    "                model_vocab_size=MODEL_VOCAB_SIZE, \n",
    "                k_items_to_prefetch=K_PREFETCH_MODEL, \n",
    "                cache_size_percentage=LRU_CACHE_SIZE_PERCENTAGE\n",
    "            )\n",
    "            config_results[\"val_model_losses\"].append(avg_val_model_loss)\n",
    "            config_results[\"val_lru_miss_ratios\"].append(val_lru_miss_ratio_wp)\n",
    "            current_val_model_loss = avg_val_model_loss\n",
    "            current_val_lru_miss_ratio = val_lru_miss_ratio_wp\n",
    "            print(f\"Config: {config['name']}, Epoch {epoch} Validation: Avg Model Loss: {avg_val_model_loss:.4f}, Prefetcher+LRU Miss Ratio: {val_lru_miss_ratio_wp:.4f}\")\n",
    "            \n",
    "            if not math.isnan(val_lru_miss_ratio_wp) and val_lru_miss_ratio_wp < config_results[\"best_val_lru_miss_ratio\"]:\n",
    "                config_results[\"best_val_lru_miss_ratio\"] = val_lru_miss_ratio_wp\n",
    "                config_results[\"best_epoch\"] = epoch\n",
    "        else:\n",
    "            config_results[\"val_model_losses\"].append(float('nan')) \n",
    "            config_results[\"val_lru_miss_ratios\"].append(float('nan'))\n",
    "            if len(val_dataset) == 0:\n",
    "                 print(f\"Config: {config['name']}, Epoch {epoch}: Validation dataset is empty. Skipping validation.\")\n",
    "            else:\n",
    "                 print(f\"Config: {config['name']}, Epoch {epoch}: Validation dataloader not available. Skipping validation.\")\n",
    "        \n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        if not math.isnan(current_val_lru_miss_ratio):\n",
    "             configurations_pbar.set_description_str(f\"Cfgs (Best P+LRUMR for {config['name']}: {config_results['best_val_lru_miss_ratio']:.4f})\")\n",
    "        print(f\"Config: {config['name']}, Epoch {epoch} duration: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "    config_duration = time.time() - start_time_config\n",
    "    print(f\"\\nTraining for {config['name']} completed in {config_duration:.2f} seconds.\")\n",
    "    print(f\"Best Validation Prefetcher+LRU Miss Ratio for {config['name']}: {config_results['best_val_lru_miss_ratio']:.4f} at Epoch {config_results['best_epoch']}\")\n",
    "    all_results.append(config_results)\n",
    "    configurations_pbar.set_description_str(\"Configurations\") \n",
    "\n",
    "print(f\"\\n\\n{'='*20} All Configurations Processed {'='*20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4114f2f-c5d2-43bc-81e9-78a0e2ae8453",
   "metadata": {},
   "source": [
    "# 8. Summarize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1902af19-b265-4d36-948a-02c917528ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration: Config1_Baseline\n",
      "  Hyperparameters: {'name': 'Config1_Baseline', 'D_MODEL': 128, 'NUM_HEADS': 4, 'NUM_LAYERS': 3, 'D_FF': 256, 'DROPOUT': 0.1, 'LEARNING_RATE': 0.001}\n",
      "  Trainable Parameters: 2347812\n",
      "  Final Training Model Loss (Epoch 5): 0.3173\n",
      "  Final Validation Model Loss (Epoch 5): 15.5823\n",
      "  Final Validation Prefetcher+LRU Miss Ratio: 0.6835\n",
      "  Best Validation Prefetcher+LRU Miss Ratio: 0.6835 (Epoch 5)\n",
      "  Improvement over Baseline LRU: 0.0209 (2.96%)\n",
      "\n",
      "Configuration: Config2_LargerModel_LowerLR\n",
      "  Hyperparameters: {'name': 'Config2_LargerModel_LowerLR', 'D_MODEL': 256, 'NUM_HEADS': 8, 'NUM_LAYERS': 4, 'D_FF': 512, 'DROPOUT': 0.15, 'LEARNING_RATE': 0.0005}\n",
      "  Trainable Parameters: 6001572\n",
      "  Final Training Model Loss (Epoch 5): 0.3064\n",
      "  Final Validation Model Loss (Epoch 5): 14.2879\n",
      "  Final Validation Prefetcher+LRU Miss Ratio: 0.6876\n",
      "  Best Validation Prefetcher+LRU Miss Ratio: 0.6831 (Epoch 1)\n",
      "  Improvement over Baseline LRU: 0.0212 (3.01%)\n",
      "\n",
      "Configuration: Config3_SmallerModel_HigherLR_MoreDropout\n",
      "  Hyperparameters: {'name': 'Config3_SmallerModel_HigherLR_MoreDropout', 'D_MODEL': 64, 'NUM_HEADS': 2, 'NUM_LAYERS': 2, 'D_FF': 128, 'DROPOUT': 0.2, 'LEARNING_RATE': 0.002}\n",
      "  Trainable Parameters: 1045924\n",
      "  Final Training Model Loss (Epoch 5): 0.6417\n",
      "  Final Validation Model Loss (Epoch 5): 17.9205\n",
      "  Final Validation Prefetcher+LRU Miss Ratio: 0.6949\n",
      "  Best Validation Prefetcher+LRU Miss Ratio: 0.6788 (Epoch 1)\n",
      "  Improvement over Baseline LRU: 0.0256 (3.63%)\n",
      "\n",
      "Configuration: Config4_MediumModel_VariedHeads\n",
      "  Hyperparameters: {'name': 'Config4_MediumModel_VariedHeads', 'D_MODEL': 128, 'NUM_HEADS': 8, 'NUM_LAYERS': 3, 'D_FF': 256, 'DROPOUT': 0.1, 'LEARNING_RATE': 0.001}\n",
      "  Trainable Parameters: 2347812\n",
      "  Final Training Model Loss (Epoch 5): 0.3104\n",
      "  Final Validation Model Loss (Epoch 5): 15.0231\n",
      "  Final Validation Prefetcher+LRU Miss Ratio: 0.6922\n",
      "  Best Validation Prefetcher+LRU Miss Ratio: 0.6886 (Epoch 4)\n",
      "  Improvement over Baseline LRU: 0.0157 (2.23%)\n"
     ]
    }
   ],
   "source": [
    "for result in all_results:\n",
    "    print(f\"\\nConfiguration: {result['config_name']}\")\n",
    "    print(f\"  Hyperparameters: {result['hyperparameters']}\")\n",
    "    print(f\"  Trainable Parameters: {result['num_parameters']}\")\n",
    "    if result['train_losses']: \n",
    "        print(f\"  Final Training Model Loss (Epoch {len(result['train_losses'])}): {result['train_losses'][-1]:.4f}\")\n",
    "        if result['val_lru_miss_ratios'] and not math.isnan(result['val_lru_miss_ratios'][-1]):\n",
    "             print(f\"  Final Validation Model Loss (Epoch {len(result['val_model_losses'])}): {result['val_model_losses'][-1]:.4f}\")\n",
    "             print(f\"  Final Validation Prefetcher+LRU Miss Ratio: {result['val_lru_miss_ratios'][-1]:.4f}\")\n",
    "        print(f\"  Best Validation Prefetcher+LRU Miss Ratio: {result['best_val_lru_miss_ratio']:.4f} (Epoch {result['best_epoch']})\")\n",
    "        \n",
    "        if not math.isnan(baseline_lru_miss_ratio) and not math.isnan(result['best_val_lru_miss_ratio']):\n",
    "            improvement = baseline_lru_miss_ratio - result['best_val_lru_miss_ratio']\n",
    "            improvement_percent = (improvement / baseline_lru_miss_ratio) * 100 if baseline_lru_miss_ratio > 0 else 0 \n",
    "            print(f\"  Improvement over Baseline LRU: {improvement:.4f} ({improvement_percent:.2f}%)\")\n",
    "    else:\n",
    "        print(\"  Training was not run for this configuration (e.g., empty dataloader).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09b8e4f7-b25d-4f5b-b461-680b94e7ccbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Overall Best Configuration (based on Prefetcher+LRU Miss Ratio) ---\n",
      "Name: Config3_SmallerModel_HigherLR_MoreDropout\n",
      "Best Validation Prefetcher+LRU Miss Ratio: 0.6788 at Epoch 1\n",
      "Hyperparameters: {'name': 'Config3_SmallerModel_HigherLR_MoreDropout', 'D_MODEL': 64, 'NUM_HEADS': 2, 'NUM_LAYERS': 2, 'D_FF': 128, 'DROPOUT': 0.2, 'LEARNING_RATE': 0.002}\n",
      "Trainable Parameters: 1045924\n",
      "  Improvement over Baseline LRU: 0.0256 (3.63%)\n"
     ]
    }
   ],
   "source": [
    "best_overall_config_result = None\n",
    "if all_results:\n",
    "    valid_results = [r for r in all_results if r['best_val_lru_miss_ratio'] != float('inf') and \\\n",
    "                     r['best_val_lru_miss_ratio'] is not None and \\\n",
    "                     not math.isnan(r['best_val_lru_miss_ratio'])]\n",
    "    if valid_results:\n",
    "        best_overall_config_result = min(valid_results, key=lambda x: x['best_val_lru_miss_ratio'])\n",
    "\n",
    "if best_overall_config_result:\n",
    "    print(f\"\\n--- Overall Best Configuration (based on Prefetcher+LRU Miss Ratio) ---\")\n",
    "    print(f\"Name: {best_overall_config_result['config_name']}\")\n",
    "    print(f\"Best Validation Prefetcher+LRU Miss Ratio: {best_overall_config_result['best_val_lru_miss_ratio']:.4f} at Epoch {best_overall_config_result['best_epoch']}\")\n",
    "    print(f\"Hyperparameters: {best_overall_config_result['hyperparameters']}\")\n",
    "    print(f\"Trainable Parameters: {best_overall_config_result['num_parameters']}\")\n",
    "    \n",
    "    if not math.isnan(baseline_lru_miss_ratio):\n",
    "        improvement = baseline_lru_miss_ratio - best_overall_config_result['best_val_lru_miss_ratio']\n",
    "        improvement_percent = (improvement / baseline_lru_miss_ratio) * 100 if baseline_lru_miss_ratio > 0 else 0\n",
    "        print(f\"  Improvement over Baseline LRU: {improvement:.4f} ({improvement_percent:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nCould not determine an overall best configuration (e.g., no valid validation results).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
